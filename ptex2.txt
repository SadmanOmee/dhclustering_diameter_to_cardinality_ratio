\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{float} %new
\usepackage{stfloats} %new
%\usepackage{ragged2e} %new
\usepackage{multirow} %new
\usepackage[font={footnotesize}]{caption} %new
\usepackage{subcaption} %new
\captionsetup[table]{textfont={sc,footnotesize}, labelfont=footnotesize, labelsep=newline, justification=centering} %new for mimicing ieee table style
\usepackage{diagbox} %new
\usepackage{mathtools} %new
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{mathabx} %new
%\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e} %new
\usepackage[noend]{algpseudocode} %new
\usepackage{cleveref} %new
\usepackage{flushend} %new for balancing columns of last page
\DeclarePairedDelimiter\abs{\lvert}{\rvert} %new
\let\oldnl\nl% Store \nl in \oldnl new
\newcommand{\nonl}{\renewcommand{\nl}{\let\nl\oldnl}}% Remove line number for one line new
\newcommand{\R}{\mathbb{R}} %new
\newcommand{\Z}{\mathbb{Z}} %new

%\newtheorem{lemma}{Lemma} %new
\usepackage{amsthm}
\theoremstyle{plain}% default
\newtheorem{thm}{Theorem}
\newtheorem{lem}[thm]{Lemma}

%\renewcommand{\thesubfigure}{\roman{subfigure}} %new
\makeatletter
\newcommand{\removelatexerror}{\let\@latex@error\@gobble}
\makeatother
%\renewcommand{\baselinestretch}{2}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

% \title{Conference Paper Title*\\
% {\footnotesize \textsuperscript{*}Note: Sub-titles are not captured in Xplore and
% should not be used}
% \thanks{Identify applicable funding agency here. If none, delete this.}
% }

\title{A Divisive Hierarchical Clustering Algorithm to Find Clusters with Smaller Diameter to Cardinality Ratio}

% \author{\IEEEauthorblockN{Sadman Sadeed Omee}
% \IEEEauthorblockA{\textit{Graph Drawing and Information Visualization Laboratory} \\
% \textit{Department of Computer Science and Engineering}\\
% \textit{Bangladesh University of Engineering and Technology(BUET)}\\
% Dhaka-1205, Bangladesh \\
% omee.sadman@gmail.com}
% \and
% \IEEEauthorblockN{~Md. Saidur Rahman}
% \IEEEauthorblockA{\textit{~Graph Drawing and Information Visualization Laboratory} \\
% \textit{~Department of Computer Science and Engineering}\\
% \textit{~Bangladesh University of Engineering and Technology(BUET)}\\
% ~Dhaka-1205, Bangladesh \\
% ~saidurrahman@cse.buet.ac.bd}
% }

\author{\IEEEauthorblockN{Sadman Sadeed Omee\IEEEauthorrefmark{1}, Md. Saidur Rahman\IEEEauthorrefmark{2}}
\IEEEauthorblockA{\textit{Graph Drawing and Information Visualization Laboratory} \\
\textit{Department of Computer Science and Engineering}\\
\textit{Bangladesh University of Engineering and Technology (BUET)}\\
Dhaka-1205, Bangladesh \\
\IEEEauthorrefmark{1}omee.sadman@gmail.com, \IEEEauthorrefmark{2}saidurrahman@cse.buet.ac.bd}
}

\maketitle

\begin{abstract}
%This document is a model and instructions for \LaTeX. This and the IEEEtran.cls file define the components of your paper [title, text, heads, etc.]. *CRITICAL: Do Not Use Symbols, Special Characters, Footnotes, or Math in Paper Title or Abstract.
\boldmath
Given a point set $S$ of $n$ points on a $2$-dimensional plane and a positive integer $k$, we are asked to split $S$ into $k$ clusters such that the maximum diameter to cardinality ratio among all clusters is minimized. In this paper we give an $O(nk \log n)$ time divisive hierarchical clustering algorithm for finding such clusters which uses two different greedy heuristics at each iteration. 
%For measuring efficiency of our algorithm, we use the Adjusted Rand Index(ARI) measure which is a very effective external evaluation criterion for evaluating clustering results. 
We compare the performance of our algorithm with that of the well known and widely used $k$-means clustering using two similarity metrics and find some cases where our algorithm performs better than $k$-means algorithm. We also test our algorithm on different benchmark datasets where the ``ground truth" labels are known and show that our algorithm outperforms $k$-means clustering in almost every case. We also perform experiments with increasing value of $k$ on another benchmark dataset and show that our algorithm performs better than the $k$-means clustering.
\end{abstract}

\begin{IEEEkeywords}
Divisive hierarchical clustering, Data mining, Unsupervised learning, Optimization criterion, $k$-means
\end{IEEEkeywords}

\section{Introduction}
Clustering is arguably the most important section of unsupervised learning where data are partitioned into sensible groups. The goals of clustering are to divide an unlabelled data set into separate groups according to their similarity, to find underlying structure in data and to organize and summarize it through cluster prototypes~\cite{jain2010data}. Clustering algorithms are used heavily in various sectors including pattern recognition and information retrieval, image segmentation, business and marketing, genetics, medical imaging etc.

Finding proper and appropriate clusters is considered a very complex problem. In fact, clustering is regarded as a harder and more challenging problem than classification~\cite{jain2010data}.

\subsection{Problem Definition} \label{sub:pd}
Given a point set $S$ consisting of $n$ points lying on a $2$-dimensional plane and a positive integer $k$, i.e., $S$ = $\{p_1, p_2, p_3, \ldots, p_n\}$, where $p_i \in \R^2$ for $i = 1, 2, \ldots, n$ and $k \in \Z$, we consider the problem of partitioning the points of $S$ into $k$ clusters $C_1, C_2, \ldots, C_k$, where $\bigcup_{c=1}^{k} C_{c} = S$ and $C_i \cap C_j = \phi$ for $i,j = 1, 2, \ldots, k$ and $i \neq j$, so as to minimize the maximum diameter to cardinality ratio among all clusters. Our algorithm starts by putting all the points in a single cluster and then recursively picks the cluster with the largest diameter to cardinality ratio and divides it into two clusters so that this optimization criterion is satisfied.

\subsection{Related Works}
Divisive hierarchical clustering algorithm starts with a single cluster composed of all data points initially. As it is a top-down approach, it recursively splits the cluster into smaller clusters. A bipartition is needed to perform at each step of a divisive hierarchical clustering algorithm of one of the currently available clusters. So naturally one of the possible approaches is to check all the possible bipartitions and choose one of them according to some criterion. Partitioning a set of $n$ objects into two non-empty subsets can be done in $2^{n - 1} - 1$ ways and Edwards and Cavalli-Sforza~\cite{edwards1965method} adopted this approach and they chose the one having the smallest intra-cluster sum of squares among all the possible bipartitions. But checking all possible bipartitions is computationally very expensive.

Macnaughton-Smith \textit{et al.}~\cite{macnaughton1964dissimilarity} and Kaufman and Rousseeuw~\cite{kaufman2009finding} both constructed iterative divisive clustering procedures with restricted number of bipartitions. Although these algorithms perform well, they are often sensitive to outliers.

Gu{\'e}noche \textit{et al.}~\cite{guenoche1991efficient} studied two divisive hierarchical clustering algorithms with the diameter criterion - an implementation of Hubert's method~\cite{hubert1973monotone} and a refinement of Rao's method~\cite{rao1971cluster}. Divisive hierarchical clustering algorithms with diameter criterion recursively pick the cluster with the maximum diameter among the currently existing clusters at that moment and split that cluster into two clusters. The problem with this criterion is that sometimes it results in a \textit{dissection effect}~\cite{cormack1971review,hansen1997cluster,monma1989partitioning}. When objects very similar to one another are assigned to separate clusters by the clustering algorithm instead of assigning them to the same cluster, it is called the dissection effect.

Some monothetic divisive algorithms have also been proposed. Monothetic divisive algorithms divides clusters using one feature or variable at a time. Williams and Lambert~\cite{williams1959multivariate} first proposed a monothetic divisive clustering methods. Lance and Williams~\cite{lance1968note} later proposed another monothetic method. But both these methods are designed particularly for binary data. Some other monothetic divisive hierarchical methods are described in \cite{chavent1998monothetic} and \cite{chavent2007divclus}. However, monothetic divisive clustering methods are not feasible for all types of data. Using one feature to partition entities is not always a good idea when there are data with a lot of diverse features.

Xiong \textit{et al.}~\cite{xiong2012dhcc} presented a divisive hierarchical clustering algorithm based on multiple correspondence analysis (MCA). But it is designed focusing only for categorical data.

\subsection{Our Contributions}
In this paper we devise an $O(nk \log n)$ time divisive hierarchical clustering algorithm for partitioning a point set $S$ consisting of $n$ points on a $2$-dimensional plane into $k$ clusters such that the maximum diameter to cardinality ratio among all clusters is minimized. We compare the performance of our algorithm with that of the widely used $k$-means clustering~\cite{lloyd1982least}. We find out the cases where $k$-means performs poorly but our algorithm excels. We then experiment our algorithm's performance with two different distance metrics against that of $k$-means clustering on some benchmark datasets and compare the results using the adjusted rand index measure that validates our algorithm's efficiency. Finally we experiment the the efficiency of both our algorithm and $k$-means clustering on increasing value of $k$ on another benchmark dataset and show that our algorithm performs better than $k$-means clustering.

The rest of the paper is organized accordingly. Section~\ref{s:pr} consists of some preliminary terms. Section~\ref{s:a} describes the whole algorithm. Section~\ref{expa} deals with the experiments, results and analysis of our algorithm and comparison with $k$-means clustering. Finally we conclude our paper in Section~\ref{s:fw} mentioning some extensions and future works.

\section{Preliminaries} \label{s:pr}
Given a cluster $C$ consisting of $n$ points $p_1, p_2, \ldots, p_n$, where $p_i \in \R^d$ for $i = 1, 2, \ldots, n$, the \textit{centroid} $c$ of $C$ is a $d$-dimensional vector which is the arithmetic mean position of all the points of $C$. Centroid $c$ can be expressed as follows $-$
\begin{equation*}
   c = \frac{1}{n} \sum_{i=1}^{n} p_i
\end{equation*}

The \textit{diameter} of a cluster is the maximum distance between any two points of the cluster. Let, $C$ be a cluster of $n$ points, where $C = \{p_1, p_2, p_3, \ldots, p_n\}$. Suppose, $dist(p_i, p_j)$ be the distance between points $p_{i}$ and $p_{j}$ of $C$. Then the diameter $d(C)$ of $C$ is as follows $-$
\begin{equation*}
    %d = \forall_{i\in n, j \in n} max\big( distance(p_i, p_j)\big)
    d(C) = \max_{p_i, p_j \in C} dist(p_i, p_j)
\end{equation*}

The \textit{cardinality} of a set or cluster is the number of elements it contains. We denote the cardinality of a cluster $C$ by $\vert C \vert$.

\textit{$k$-means} is an iterative clustering method where $k$ pre-defined centroids are used to create $k$ clusters. Each data is assigned to the cluster with the nearest mean. The objective function of $k$-means algorithm is to minimize intra-cluster variance i.e., sum of squared error (SSE). For every data point $p$ and centroid $c$, the sum squared error can be defined as follows where $p_j^{i}$ is the \(j^{th}\) data point assigned to \(i^{th}\) cluster.
\begin{equation*}
  SSE = \sum_{i=1}^{k} \sum_{j=1}^{n}  \abs[\Big]{\abs[\Big]{p_j^{i} - c_i}}^{2}
\end{equation*}

Given a set of points, the \textit{convex hull} of the point set is defined as the smallest convex polygon that circumscribes all the points of the set. Convex hull is used in our algorithm for farthest point pair query of a point set.

\textit{$kd$-tree} is a binary tree data structure where each leaf node represents a point in a $k$-dimensional space. $kd$-tree is used in our algorithm for nearest neighbor search.

For two $2$-dimensional points $ M(x_1, y_1)$ and $N(x_2, y_2)$, the Euclidean. Manhattan and Chebyshev distance can be defined as follows $-$
\begin{align*}
  Euclidean(M, N) &= \sqrt{\Big(x_1 - x_2\Big)^{2} + \Big(y_1 - y_2\Big)^{2}} \\
  Manhattan(M, N) &= \abs[\Big]{\abs[\Big]{x_1 - x_2}} + \abs[\Big]{\abs[\Big]{y_1 - y_2}} \\
  Chebyshev(M, N) &= max\Bigg( \abs[\Big]{\abs[\Big]{x_1 - x_2}}, \abs[\Big]{\abs[\Big]{y_1 - y_2}} \Bigg)
\end{align*}

We define a term \textit{ratio value} in this paper. This value is the ratio of the diameter to cardinality of a cluster. This ratio value is used as the criterion for splitting clusters in our divisive hierarchical clustering algorithm. Our objective is to minimize the maximum ratio value among all clusters.

\begin{table}[!b]
\centering
\caption{Contingency Table for Comparing Two Different Clustering Output G and C of Set S.}\label{t:ct}
\setlength\tabcolsep{1.5pt}
\begin{tabular}{|c|ccccc|c|}
\hline
\diagbox{\textbf{Clustering $G$}}{\textbf{Clustering $C$}} & $c_1$ & $c_2$ & $c_3$ & \ldots & $c_N$ & \textbf{Total}\\
\hline
$g_1$ & $p_{11}$ & $p_{12}$ & $p_{13}$ & \ldots & $p_{1N}$ & $p_{1.}$\\
$g_2$ & $p_{21}$ & $p_{22}$ & $p_{23}$ & \ldots & $p_{2N}$ & $p_{2.}$\\
$g_3$ & $p_{31}$ & $p_{32}$ & $p_{33}$ & \ldots & $p_{3N}$ & $p_{3.}$\\
\vdots & \vdots & \vdots & \vdots & $\ddots$ & \vdots & \vdots\\
$g_M$ & $p_{M1}$ & $p_{M2}$ & $p_{M3}$ & \ldots & $p_{MN}$ & $p_{M.}$\\
\hline
\textbf{Total} & $p_{.1}$ & $p_{.2}$ & $p_{.3}$ & \ldots & $p_{.N}$ & $p_{..} = n$\\
\hline
\end{tabular}
\end{table}

William M. Rand~\cite{rand1971objective} proposed Rand Index(RI) to define a measure of similarity between two different clusterings of a specific point set. One of them is usually the ground truth clustering so that one can measure how accurate another clustering algorithm's result is to that of the ground truth clustering. Suppose a set of $n$ objects $S = \{\delta_1, \delta_2, \delta_3, \ldots, \delta_n\}$ is given. Let, $G =  \{g_1, g_2, g_3, \ldots, g_M\}$ and $C = \{c_1, c_2, c_3, \ldots, c_N\}$ be two different clusterings of $S$ where $C$ is the clustering algorithm's result and $G$ is the ``ground truth" classification. We construct the contingency table in Table~\ref{t:ct} to show the overlap between $G$ and $C$. Each entry $p_{ij}$ in Table~\ref{t:ct} indicates total number of objects both $g_i$ and $c_j$ contains. Let $a$ be the total pairs of objects those are in the same cluster in both $G$ and $C$, $b$ be the total pairs of objects those are in the same cluster in $G$ but not in the same cluster in $C$, $c$ be the total pairs of objects those are in the same cluster in $C$ but not in the same cluster in $G$ and $d$ be the total pairs of objects those are in different clusters in both $G$ and $C$. These variables can be expressed by the terms of Table~\ref{t:ct} by the following equations.

\begin{align*}
    a &= \sum_{i=1}^{M} \sum_{j=1}^{N} \binom{p_{ij}}{2}\\
    b &= \sum_{i=1}^{M} \binom{p_{i.}}{2} - a\\
    c &= \sum_{j=1}^{N} \binom{p_{.j}}{2} - a\\
    d &= \binom{n}{2} - a - b - c
\end{align*}

Then RI can be calculated by the following equation.
\begin{equation*}
    RI = \frac{a + d}{a + b + c + d}
\end{equation*}
The range of RI is between $0$ to $1$ where higher values indicate good clustering and lower values indicate bad clustering.

However, there are some problems with this RI measure, such as the expected value of RI of two random clusterings never happens to be a constant value, it shows high variability~\cite{fowlkes1983method} and it is very sensitive to the total clusters computed in each clustering~\cite{morey1984measurement}. Hubert and Arabie~\cite{hubert1985comparing} proposed the Adjusted Rand Index(ARI) for removing these problems by assuming the generalized hypergeometrical distribution as a model of randomness. ARI can be expressed by the following equation.

%It indicates the percentage of correct labels the clustering algorithm put on the objects of the point set.
\begin{equation*}
ARI = \frac{\binom{n}{2}(a+d)-[(a+b)(a+c)+(b+d)(c+d)]}{\binom{n}{2}^2-[(a+b)(a+c)+(b+d)(c+d)]}
\end{equation*}

\section{The Algorithm} \label{s:a}
In this section, we describe our algorithm. Let $S$ = $\{p_1, p_2, p_3, \ldots, p_n\}$ be a point set of $n$ points on a $2$-dimensional plane and $k$ be a positive integer. The goal of any clustering algorithm is to divide the point set into meaningful clusters, i.e., similar points into same cluster and dissimilar points into separate clusters. There are many ways to measure similarity between two points. In this paper we select the distance between two points as similarity measure. We propose a new optimization criterion for our divisive hierarchical clustering algorithm, that is to partition $S$ into $k$ clusters such that the maximum diameter to cardinality ratio among all clusters is minimized. We divide our algorithm into four major steps. They are the briefly described below.
\begin{enumerate}
\item Initially dividing the point set $S$ into two clusters $C_1$ and $C_2$ in such a way that number of points in both the clusters are as close as possible. We name it the \textit{initial divide} step.
\item Finding points those are actually nearer to the centroid of $S$ than the centroid of the cluster they are assigned to. We remove those points from their corresponding cluster and store it in a temporary cluster $C_{temp}$. We name it the \textit{temporary cluster creation} step. \label{step:2}
\item Applying a greedy heuristic and merging the temporary cluster $(C_{temp})$ found in step~\ref{step:2} with one of the clusters between $C_1$ and $C_2$. We use the greedy heuristic to determine which cluster between $C_1$ and $C_2$ we need to merge $C_{temp}$ with. We call it the \textit{merge by greedy heuristic} step.
\item Finally we remove all the points of $C_1$ those are nearer to the centroid of $C_2$ than that of $C_1$ and assign them to $C_2$. Then we do the same for $C_2$, that is, we remove all the points of $C_2$ those are nearer to the centroid of $C_1$ than that of $C_2$ and assign them to $C_1$. We call it the \textit{filtering} step.
\end{enumerate}

These four steps creates two clusters $C_1$ and $C_2$ from point set $S$. Our objective is to get $k$ clusters. According to the rule of a divisive hierarchical clustering algorithm, we need to repeat the above mentioned four steps in either $C_1$ or $C_2$ and so on to divide into more clusters. We need to use these steps $k - 1$ times for getting $k$ clusters. Our algorithm selects the cluster that has the maximum diameter to cardinality ratio among the existing clusters for partitioning in each iteration as our objective is to minimize the maximum ratio value among all clusters. In Subsection~\ref{idv} - \ref{flt}, we discuss the four major steps in details.

\subsection{Initial Divide} \label{idv}
Our algorithm first divides $S$ into two clusters $C_1$ and $C_2$. Initially it considers the whole point set $S$ as a single cluster. Let, that cluster be $C$. The algorithm starts by finding the two farthest points $f_1$ and $f_2$ of $C$ and assigning $f_1$ to $C_1$ and $f_2$ to $C_2$ and removing those points from $C$. Then the algorithm computes nearest points of $f_1$ and $f_2$ in $C$ and assigns them to $C_1$ and $C_2$, respectively and removes those points from $C$. The process is repeated until $C$ is empty. A formal description of the algorithm is given in Algorithm~\ref{a:idv}.

\begin{algorithm}[!t]
\SetAlgoLined
\nonl\textbf{Input: }A cluster $C$ \\
\nonl\textbf{Output: }Two clusters $C_1$ and $C_2$ \\
 $C_1, C_2 \gets \phi$ \\
 $f_1, f_2 \gets$ farthest point pair of $C$ \\
 $C \gets C$ \textbackslash $\{f_1, f_2\}$ \\
 $C_1 \gets C_1 \cup \{f_1\}$ \\
 $C_2 \gets C_2 \cup \{f_2\}$ \\
 $q_1 \gets f_1$ \\
 $q_2 \gets f_2$ \\
 \While{$C$ is not empty}{
  $x_1 \gets$ nearest point of $q_1$ of $C$ \\
  $C \gets C$ \textbackslash $\{x_1\}$ \\
  $C_1 \gets C_1 \cup \{x_1\}$ \\
  $q_1 \gets x_1$ \\
  \If{$C$ is not empty}{
    $x_2 \gets$ nearest point of $q_2$ of $C$ \\
    $C \gets C$ \textbackslash $\{x_2\}$ \\
    $C_2 \gets C_2 \cup \{x_2\}$ \\
    $q_2 \gets x_2$ \\
   }
 }
 \Return $C_1, C_2$
 \caption{$InitialDivide(C)$}
 \label{a:idv}
\end{algorithm}

Computing farthest point pair of a $2$-dimensional point set takes total $O(n \log n)$ time by rotating calipers method~\cite{toussaint1983solving}. This method first requires computing the convex hull of the point set which takes $O(n \log n)$ time. Then it searches for the two farthest points in $O(n)$ time with the rotating calipers. So in total it takes $O(n \log n)$ time. Nearest point query of a specific point within a point set takes $O(\log n)$ time in the expected case by using a $kd$-tree~\cite{friedman1977algorithm}.

\subsection{Temporary Cluster Creation} \label{tcc}
In this step the algorithm creates a temporary cluster $C_{temp}$. Some points may lie closer to the centroid of the whole point set rather than the centroid of the cluster it is assigned to in the previous step. Those points are temporarily assigned to $C_{temp}$ and removed from their respective cluster. 
%If the points of $C_{temp}$ comes from only one of the clusters between the two found in the previous step, then this step is not very significant. But if the points come from both the clusters, then this step is really vital. 
A formal description of the algorithm is presented in Algorithm~\ref{a:tcc}.

%---------temporary cluster creation-------%
\begin{algorithm}
\SetAlgoLined
\nonl\textbf{Input: }Two clusters $C_1$ and $C_2$ \\
\nonl\textbf{Output: }Updated Clusters $C_1, C_2$ and a temporary cluster $C_{temp}$\\
 $c_1 \gets$ centroid of $C_1$ \\
 $c_2 \gets$ centroid of $C_2$ \\
 $c \gets$ centroid of $C_1$ and $C_2$ combined \\ %\Comment{centroid of $C_1$ and $C_2$ combined} \\
 $C_{temp} \gets \phi$ \\
 \For{$i \gets C_1.begin$ to $C_1.end$}{
  $x \gets i^{th}$ point of $C_1$ \\
  %$dist$\underline{{ }{ }}$x$\underline{{ }{ }}$c_1 \gets$ distance between $x$ and $c_1$ \\
  $dist(x, c_1) \gets$ distance between $x$ and $c_1$ \\
  %$dist$\underline{{ }{ }}$x$\underline{{ }{ }}$cent \gets$ distance between $x$ and $cent$ \\
  $dist(x, c) \gets$ distance between $x$ and $c$ \\
  \If{$dist(x, c) < dist(x, c_1)$}{
   $C_{temp} \gets C_{temp} \cup \{x\}$ \\
   $C_1 \gets C_1$ \textbackslash $\{x\}$ \\
   }
 }
 \For{$i \gets C_2.begin$ to $C_2.end$}{
  $x \gets i^{th}$ point of $C_2$ \\
  %$dist$\underline{{ }{ }}$x$\underline{{ }{ }}$c_2 \gets$ distance between $x$ and $c_2$ \\
  $dist(x, c_2) \gets$ distance between $x$ and $c_2$ \\
  %$dist$\underline{{ }{ }}$x$\underline{{ }{ }}$cent \gets$ distance between $x$ and $cent$ \\
  $dist(x, c) \gets$ distance between $x$ and $c$ \\
   \If{$dist(x, c) < dist(x, c_2)$}{
   $C_{temp} \gets C_{temp} \cup \{x\}$ \\
   $C_2 \gets C_2$ \textbackslash $\{x\}$ \\
   }
 }
 \Return $C_1, C_2, C_{temp}$
 \caption{$CreateTemporaryCluster(C_1, C_2)$}
 \label{a:tcc}
\end{algorithm}
\vspace{0.5em}
%---------temporary cluster creation-------%

We denote the distance between two point $x_1$ and $x_2$ by $x_1x_2$. Let, $C$ be a cluster and $c$ be its centroid. Assume that $f$ is the farthest from from $c$ within all the points of $C$. Then we define the \textit{range} of cluster $C$ as the circular area taking $cf$ as the radius. Any point that is inside this circle is within the range of cluster $C$. The intuition of this step comes from Lemma~\ref{le:1} where the meaning of $C_1$ and $C_2$ is stated in Algorithm~\ref{a:tcc}.

\begin{lem} \label{le:1}
Let, $p_1 \in C_1$ and $p_2 \in C_2$ and $c_1$ and $c_2$ be their centroid, respectively. Suppose, they are assigned to $C_{temp}$ by Algorithm~\ref{a:tcc}. Then for some point $f_1$ collinear with $p_1$ and $c_1$  and some point $f_2$ collinear with $p_2$ and $c_2$ where $p_1c_1 = f_1c_1$ and $p_2c_2 = f_2c_2$, it can be stated that, $p_1p_2$ is at least smaller than either $p_1f_1$ or $p_2f_2$.
\end{lem}

\begin{IEEEproof}
%The convex hull of $C_1$ and $C_2$ are shown in Figure[1111]
Let, $c$ be the centroid of the points of $C_1$ and $C_2$ combined. We choose $f_1$($f_2$) in such a way that $c_1$($c_2$) is equidistant from $p_1$($p_2$) and $f_1$($f_2$) and $p_1$($p_2$), $c_1$($c_2$), $f_1$($f_2$) are collinear. The reason is described below.

We consider a circle with $p_1c_1$ as radius. If $p_1$ is the farthest point of $C_1$ from $c_1$, then the circle will cover all the points of $C_1$. Else, at least point of $C_1$ will be outside of the circle. So in either case, $f_1$ is in the range of $C_1$. So we can consider a point at $f_1$ and if that point belonged the actual point set, it would have been assigned to $C_1$ as it is well inside the range of $C_1$. Same goes for $f_2$ being in the range of $C_2$.

%Again, the centroid of a set of points lies interior to the convex hull of the point set[1111], so we can say that, $p_1c_1 < p_1f_1$ and $p_2c_2 < p_2f_2$. So combining these equations, we get,
Now as $p_1$ and $p_2$ are assigned to $C_{temp}$, so $p_1c < p_1c_1$ and $p_2c < p_2c_2$. Combining these,
\begin{equation} \label{eq:1}
p_1c + p_2c < p_1c_1 + p_2c_2
\end{equation}
We consider $3$ cases.
\newline \textbf{Case 1:} $p_1c_1 > p_2c_2$
\newline Equation \eqref{eq:1} becomes,
\begin{align}
p_1c + p_2c &< p_1c_1 + p_1c_1 \nonumber\\
&= p_1c_1 + f_1c_1 \nonumber\\
&= p_1f_1 \nonumber\\
\therefore p_1c + p_2c &< p_1f_1 \label{eq:2}
\end{align}
\noindent Now $p_1p_2 < p_1c + p_2c$, because of triangle inequality. Then \eqref{eq:2} becomes,
\begin{align}
p_1p_2 < p_1c + p_2c &< p_1f_1 \nonumber \\
\therefore p_1p_2 &< p_1f_1 \label{eq:3}
\end{align}
\newline \textbf{Case 2:} $p_1c_1 < p_2c_2$
\newline Similar to case $1$ we prove it for $f_2$ this time. Finally we get,
\begin{equation} \label{eq:4}
 p_1p_2 < p_2f_2
\end{equation}
\newline \textbf{Case 3:} $p_1c_1 = p_2c_2$
\newline Both \eqref{eq:3} and \eqref{eq:4} can be proved true in this case. So we get,
\begin{equation} \label{eq:5}
 p_1p_2 < p_1f_1~and~p_1p_2 < p_2f_2
\end{equation}
Combining \eqref{eq:3}, \eqref{eq:4} and \eqref{eq:5}, we can say that, $p_1p_2$ is at least smaller than either $p_1f_1$ or $p_2f_2$.
\end{IEEEproof}

Lemma~\ref{le:1} indicates that despite having a larger distance, if $(p_1, f_1)$ pair or $(p_2, f_2)$ pair deserve to be in the same cluster, then it can be argued that $(p_1, p_2)$ pair also deserves to be in the same cluster. That is why all the points assigned to $C_{temp}$ are merged with either $C_1$ or $C_2$ in the next step so that they remain in the same cluster.

\subsection{Merge by Greedy Heuristic} \label{mgh}
In this step we give a greedy heuristic to merge cluster $C_1$ or $C_2$ with cluster $C_{temp}$ obtained from the previous step. So final cluster pairs after this step would be either $(C_1 + C_{temp}, C_2)$ or $(C_1, C_2 + C_{temp})$. We denote $(C_1 + C_{temp})$ as $C_1^{\Asterisk}$ and $(C_2 + C_{temp})$ as $C_2^{\Asterisk}$.

First we describe the greedy heuristic. We calculate the ratio value of all $C_1, C_2, C_1^{\Asterisk}$ and $C_2^{\Asterisk}$. If the sum of ratio values of $C_1$ and $C_2^{\Asterisk}$ is less than the sum of ratio values of $C_2$ and $C_1^{\Asterisk}$ we merge $C_{temp}$ with $C_2$. Similarly, we merge $C_{temp}$ with $C_1$ if the sum of ratio value of $C_2$ and $C_1^{\Asterisk}$ is less than the sum of ratio values of $C_1$ and $C_2^{\Asterisk}$. In every iteration we perform the greedy operation of selecting between $C_1$ and $C_2$ for merging with $C_{temp}$ in such a way that the sum of ratio value of the two output clusters of this step is minimized. That is because, a lower sum of ratio value means the points inside the clusters are more compact which is one of the desired properties of a good clustering. We present a helper function for calculating the ratio value in Algorithm~\ref{a:cr} and a formal description of this step is given in Algorithm~\ref{a:mgh}.
%This value gives the idea of the spacing of the points in every direction within the cluster.

\begin{algorithm}
\SetAlgoLined
\nonl\textbf{Input: }A cluster $C$ \\
\nonl\textbf{Output: } Diameter to cardinality ratio of C \\
 $d \gets$ diameter of $C$ \\ 
 $ratio \gets \frac{d}{\vert C \vert}$ \\
 \Return $ratio$
 \caption{$CalculateRatio(C)$}
 \label{a:cr}
\end{algorithm}

\begin{algorithm}
\SetAlgoLined
\nonl\textbf{Input: }Three clusters $C_1, C_2$ and $C_{temp}$ \\
\nonl\textbf{Output: }Updated Clusters $C_1$ and $C_2$ \\
 $C_1^{\Asterisk} \gets C_1 \cup C_{temp}$ \\
 $C_2^{\Asterisk} \gets C_2 \cup C_{temp}$ \\
 %$diam_1 \gets$ diameter of $C_1$ \\
 %$diam_2 \gets$ diameter of $C_2$ \\
 %$diam_1^\ast \gets$ diameter of $C_1^\Asterisk$ \\
 %$diam_2^\ast \gets$ diameter of $C_2^\Asterisk$ \\
 %$ratio_1 \gets \frac{diam_1^{\ast}}{C_1^{\Asterisk}.length} + \frac{diam_2}{C_2.length}$ \\
 %$ratio_2 \gets \frac{diam_1}{C_1.length} + \frac{diam_2^{\ast}}{C_2^{\Asterisk}.length}$ \\
 $ratio_1 \gets CalculateRatio(C_1^{\Asterisk}) + CalculateRatio(C_2)$ \\
 $ratio_2 \gets CalculateRatio(C_1) + CalculateRatio(C_2^{\Asterisk})$ \\
 \eIf{$ratio_1 \leq ratio_2$}{
   $C_1 \gets C_1^\Asterisk$ \\
   }{
   $C_2 \gets C_2^\Asterisk$ \\
   }
   \textbf{delete} $C_1^\Asterisk, C_2^\Asterisk$ and $C_{temp}$ \\
 \Return $C_1, C_2$
 \caption{$MergeByGreedyHeuristic(C_1, C_2, C_{temp})$}
 \label{a:mgh}
\end{algorithm}

\subsection{Filtering} \label{flt}
We got the updated $C_1$ and $C_2$ in the last step. The filtering step is plain and simple. It takes all the points of $C_1$ those are nearer to the centroid of $C_2$ than that of $C_1$ and assigns them to $C_2$. Similarly, it takes all the points of $C_2$ those are nearer to the centroid of $C_1$ than that of $C_2$ and assigns them to $C_1$. A formal description of the algorithm is given in Algorithm~\ref{a:flt}.

\begin{algorithm}
\SetAlgoLined
\nonl\textbf{Input: }Two clusters $C_1$ and $C_2$ \\
\nonl\textbf{Output: }Updated Clusters $C_1$ and $C_2$ \\
 $c_1 \gets$ centroid of $C_1$ \\
 $c_2 \gets$ centroid of $C_2$ \\
 \For{$i \gets C_1.begin$ to $C_1.end$}{
  $x \gets i^{th}$ point of $C_1$ \\
  %$dist$\underline{{ }{ }}$c_1 \gets$ distance between $x$ and $c_1$ \\
  %$dist$\underline{{ }{ }}$c_2 \gets$ distance between $x$ and $c_2$ \\
  $dist(x, c_1) \gets$ distance between $x$ and $c_1$ \\
  $dist(x, c_2) \gets$ distance between $x$ and $c_2$ \\
  \If{$dist(x, c_1) > dist(x, c_2)$}{
   $C_1 \gets C_1$ \textbackslash $\{x\}$ \\
   $C_2 \gets C_2 \cup \{x\}$ \\
   }
 }
 \For{$i \gets C_2.begin$ to $C_2.end$}{
  $x \gets i^{th}$ point of $C_2$ \\
  %$dist$\underline{{ }{ }}$c_1 \gets$ distance between $x$ and $c_1$ \\
  %$dist$\underline{{ }{ }}$c_2 \gets$ distance between $x$ and $c_2$ \\
  $dist(x, c_1) \gets$ distance between $x$ and $c_1$ \\
  $dist(x, c_2) \gets$ distance between $x$ and $c_2$ \\
  \If{$dist(x, c_1) < dist(x, c_2)$}{
   $C_2 \gets C_2$ \textbackslash $\{x\}$ \\
   $C_1 \gets C_1 \cup \{x\}$ \\
   }
 }
 \Return $C_1, C_2$
 \caption{$Filtering(C_1, C_2)$}
 \label{a:flt}
\end{algorithm}

\subsection{Putting it all Together} \label{piat}
Here we combine all the steps and complete our divisive hierarchical algorithm. As we said earlier, Algorithm~\ref{a:idv} - \ref{a:tcc} and \ref{a:mgh} - \ref{a:flt} divides a point set into two clusters. Algorithm~\ref{a:ctc} is a combined version of all these steps which is a part of every iteration of our algorithm.

\begin{algorithm}
\SetAlgoLined
\nonl\textbf{Input: }A cluster $C$ \\
\nonl\textbf{Output: }Clustering of $C$ to split it into two new clusters $C_1$ and $C_2$ \\
 $C_1, C_2 \gets InititalDivide(C)$ \\
 $C_1, C_2, C_{temp} \gets CreateTemporaryCluster(C_1, C_2)$ \\
 $C_1, C_2 \gets MergeByGreedyHeuristic(C_1, C_2, C_{temp})$ \\
 $C_1, C_2 \gets Filtering(C_1, C_2)$ \\
 \Return $C_1, C_2$
 \caption{$CreateTwoClusters(C)$}
 \label{a:ctc}
\end{algorithm}

We discussed earlier that we select the cluster that has the maximum ratio value among the existing clusters and select it for splitting using Algorithm~\ref{a:ctc} in each iteration as our optimization criterion is to minimize this ratio value. This indicates another greedy heuristic as we are always choosing the cluster that has the maximum ratio value for splitting. For this we calculate the ratio value of each cluster. We then create a state with the cluster along with its ratio value and store it in a set. Then we extract the cluster in that set that has the maximum ratio value because our objective is to minimize the maximum ratio value among all clusters.

After $k - 1$ iterations of our algorithm, we get $k$ clusters that satisfies our optimization criterion. A formal definition of the complete divisive hierarchical clustering algorithm is presented in Algorithm~\ref{a:dhc}.

\begin{algorithm}
\SetAlgoLined
\nonl\textbf{Input: }The point set $S$ and an integer $k$ \\
\nonl\textbf{Output: }$k$ clusters of $S$ such the the maximum ratio value among all clusters is minimized \\
 $C \gets \phi$ \\
 %$d \gets$ diameter of $S$ \\ 
 %$ratio \gets \frac{d}{S.length}$ \Comment{$ratio$ value $S$} \\
 create a cluster $C_S$ with all the points of S \\
 $ratio \gets CalculateRatio(C_S)$ \\
 create a state $c$ with $C_S$ and $ratio$ \\
 insert $c$ into $C$ \\
 %foravgratio $C_{state} \gets C$ \\
 %foravgratio $minAvgRatio \gets ratio$ \\
 \For{$i \gets 1$ to $k - 1$}{
  $state \gets$ extract the state with the maximum $ratio$ $value$ from $C$ \\ 
  $cluster \gets$ extract the cluster from $
  state$ \\
  $C_1, C_2 \gets CreateTwoClusters(cluster)$ \\
  %$diam_1 \gets$ diameter of $C_1$ \\
  %$diam_2 \gets$ diameter of $C_2$ \\
  %$ratio_1 \gets \frac{diam_1}{C_1.length}$ \Comment{$ratio$ value $C_1$}\\
  %$ratio_2 \gets \frac{diam_2}{C_2.length}$ \Comment{$ratio$ value $C_2$}\\
  $ratio_1 \gets CalculateRatio(C_1)$ \\
  $ratio_2 \gets CalculateRatio(C_2)$ \\
  create a state $c_1$ with $C_1$ and $ratio_1$ \\
  insert $c_1$ into $C$ \\
  create a state $c_2$ with $C_2$ and $ratio_2$ \\
  insert $c_2$ into $C$ \\
  %foravgratio $avgRatio \gets$ calculate the average of $ratio$ values of all clusters of $C$ \\
  %foravgratio \If{$avgRatio < minAvgRatio$}{
   %foravgratio $C_{state} \gets C$ \\
   %foravgratio $minAvgRatio \gets avgRatio$ \\
   %foravgratio }
 }
 %foravgratio \textbf{delete} $C$ \\
 $clusters \gets$ extract the clusters from $C$ \\%foravgratio C_{state} instead of of C
 \textbf{delete} $C$ \\
 \Return $clusters$
 \caption{$DHClustering(S)$}
 \label{a:dhc}
\end{algorithm}

\section{Results and  Analysis} \label{expa}
The experiments of this paper are done on a computer that has an Intel(R) Core(TM) i$7$-$4510$U CPU with a processor base frequency of $2.00$ gigahertz and a maximum turbo frequency of $3.10$ gigahertz. The installed memory of the computer is $8.00$ gigabytes. The program for our algorithm was written in C$++$ language. For generating $k$-means output we used the popular machine learning library Scikit-learn for the Python programming language which has an implementation of the $k$-means clustering that uses an efficient initialization method proposed by Arthur and Vassilvitskii~\cite{arthur2006k}. In this paper we run $k$-means clustering $30$ times with different centroid seeds. Then we find the run with the minimum sum squared error and choose it as the final output. All the figures are drawn using libraries of the Python language.

Fig.~\ref{f:sim} shows step by step simulation of Algorithm~\ref{a:dhc} for a point set for $k = 2$ using euclidean distance. The points of the temporary cluster are marked by purple `$\ast$'.

Sample outputs of Algorithm~\ref{a:dhc} for a point set for $k = 3$ using manhattan distance and for another point set for $k = 2$ using chebyshev distance is shown in Fig.~\ref{f:mcoutput}. We will only use euclidean and manhattan distance metrics for the rest of the experiments of this paper.

\begin{figure}[!t]
\centering
\begin{subfigure}{.25\textwidth}
  \centering
  \includegraphics[width=.95\linewidth]{algorithm_simulation_example_a.png}
  \caption{A point set}
  \label{f:sim1}
\end{subfigure}%
\begin{subfigure}{.25\textwidth}
  \centering
  \includegraphics[width=.95\linewidth]{algorithm_simulation_example_b.png}
  \caption{Initial divide}
  \label{f:sim3}
\end{subfigure}
\begin{subfigure}{.25\textwidth}
  \centering
  \includegraphics[width=.95\linewidth]{algorithm_simulation_example_c.png}
  \caption{Temporary cluster creation}
  \label{f:sim4}
\end{subfigure}%
\begin{subfigure}{.25\textwidth}
  \centering
  \includegraphics[width=.95\linewidth]{algorithm_simulation_example_d.png}
  \caption{Merge by greedy heuristic}
  \label{f:sim5}
\end{subfigure}
\begin{subfigure}{.25\textwidth}
  \centering
  \includegraphics[width=.95\linewidth]{algorithm_simulation_example_e.png}
  \caption{Filtering}
  \label{f:sim6}
\end{subfigure}%
\begin{subfigure}{.25\textwidth}
  \centering
  \includegraphics[width=.95\linewidth]{algorithm_simulation_example_e.png}
  \caption{Final output}
  \label{f:sim6}
\end{subfigure}
\caption{Step by step simulation of Algorithm~\ref{a:dhc} for a point set for $k = 2$ using euclidean distance.}
\label{f:sim}
\end{figure}

\begin{figure}[!t]
\centering
\begin{subfigure}{.25\textwidth}
  \centering
  \includegraphics[width=.95\linewidth]{algorithm_output_manhattan_a.png}
  \caption{A point set\\~}
  \label{f:simm1}
\end{subfigure}%
\begin{subfigure}{.25\textwidth}
  \centering
  \includegraphics[width=.95\linewidth]{algorithm_output_manhattan_d.png}
  \caption{Clustering output using manhattan distance}
  \label{f:simm2}
\end{subfigure}
\begin{subfigure}{.25\textwidth}
  \centering
  \includegraphics[width=.95\linewidth]{algorithm_output_chebyshev_a.png}
  \caption{A point set\\~}
  \label{f:simc1}
\end{subfigure}%
\begin{subfigure}{.25\textwidth}
  \centering
  \includegraphics[width=.95\linewidth]{algorithm_output_chebyshev_b.png}
  \caption{Clustering output using chebyshev distance}
  \label{f:simc2}
\end{subfigure}
\caption{Sample output of Algorithm~\ref{a:dhc} for (b) $k = 3$ using manhattan distance and (d) $k = 2$ using chebyshev distance.}
\label{f:mcoutput}
\end{figure}

Our algorithm find clusters such that the maximum diameter to cardinality ratio among all clusters is minimized. This optimization criterion not only takes account of the diameter of the clusters like the diameter criterion~\cite{guenoche1991efficient,hubert1973monotone,rao1971cluster} but also the density of points within each cluster. That is why our algorithm criterion is less prone to dissection effect than those which optimize diameter criterion.

We now compare our algorithm with $k$-means clustering. Berkhin~\cite{berkhin2006survey} stated in his survey that ``The $k$-means algorithm is by far the most popular clustering tool used in scientific and industrial applications". This is why we chose $k$-means clustering for comparing. 
%Lloyd~\cite{lloyd1982least} proposed a local search solution to this problem which is the most popular version of the $k$-means algorithm.

We start by comparing their running time. For $n$ points on a $2$-dimensional plane and a total of $k$ desired clusters, our algorithm has a running time complexity of $O(nk \log n)$. We now prove Lemma~\ref{l:2}.

\begin{lem}\label{l:2}
Algorithm~\ref{a:dhc} has a running time complexity of $O(nk \log n)$.
\end{lem}
\begin{IEEEproof}
The farthest point pair calculation of a point set takes $O(n \log n)$ time. The nearest neighbor calculation of a specific point of a point set takes amortized $O(\log n)$ time. So the total running time of the initial divide step is $O(n \log n)$. The calculation of the centroid of point set and the calculation of distance between two points both take $O(n)$ time. So the temporary cluster creation step has a running time complexity of $O(n)$. The merge by greedy heuristic step takes $O(n \log n)$ time as the most costly calculation of this step is to find diameter by cardinality ratio which takes $O(n \log n)$ time. Finally the filtering step takes $O(n)$ time. Combining all these steps, we have Algorithm~\ref{a:ctc} and so it has a running time complexity of $O(n \log n)$.

Now, Algorithm~\ref{a:ctc} is a part of each iteration of our main algorithm. The iteration goes over for $k - 1$ times. The other parts of each iteration are finding the cluster with highest ratio value for splitting and finding ratio values of the two new clusters found in that iteration. So the calculations of each iteration has a running time complexity of $O(n \log n)$. So, considering $k - 1$ iterations, finally we can say that, Algorithm~\ref{a:dhc} has a running time complexity of $O(nk \log n)$.
\end{IEEEproof}

The converge of $k$-means clustering is highly researched and it is known to have a superpolynomial, i.e., exponential running time in worst case~\cite{arthur2006slow,vattani2011k}. The reason is that $k$-means can have exponentially many iterations in the worst case. As our algorithm have specific number of iterations, it can never have an exponential running time like $k$-means clustering. This proves that our algorithm has a better running time than $k$-means clustering in the worst case.

Also the accuracy of $k$-means clustering highly depends on the initialization because the iterations of $k$-means clustering can get stuck in a local minimum for bad initialization. Our algorithm does not face this problem as it produces the same output for multiple runs.

Fig.~\ref{f:kmeanfailure} shows some cases where $k$-means clustering fails to generate good clusters but our algorithm succeeds to find proper clusters. The comparison of results with these cases are shown in Table~\ref{t:comp1}. $k$-means does not generate good clusters with these cases because it tries to minimize the intra-cluster variance. On the other hand both the RI and ARI value of our algorithm with these cases is $1$ as it produces perfect clusters with these cases. $k$-means clustering performs poorly with these cases and thus have lower RI and ARI values and even negative ARI values in some cases.

We now compare our algorithm with $k$-means clustering for some benchmark datasets. We select $6$ datasets for this purpose $-$ Unbalance, S$1$, A-Sets(A$1$, A$2$, A$3$) and Birch$2$. Details about these datasets can be found in \cite{franti2018k}. We also select $2$ shape sets~\cite{franti2018clustering} for comparing.

\begin{figure}[!t]
\centering
\begin{subfigure}{.167\textwidth}
  \centering
  \includegraphics[width=\linewidth]{kmeans_failure_1a.png}
  \caption{A point set}
  \label{f:simm1}
\end{subfigure}%
\begin{subfigure}{.167\textwidth}
  \centering
  \includegraphics[width=\linewidth]{kmeans_failure_1b.png}
  \caption{Algorithm~\ref{a:dhc} output}
  \label{f:simm2}
\end{subfigure}%
\begin{subfigure}{.167\textwidth}
  \centering
  \includegraphics[width=\linewidth]{kmeans_failure_1c.png}
  \caption{$k$-means output}
  \label{f:simm3}
\end{subfigure}
\begin{subfigure}{.167\textwidth}
  \centering
  \includegraphics[width=\linewidth]{kmeans_failure_2a.png}
  \caption{A point set}
  \label{f:simm4}
\end{subfigure}%
\begin{subfigure}{.167\textwidth}
  \centering
  \includegraphics[width=\linewidth]{kmeans_failure_2b.png}
  \caption{Algorithm~\ref{a:dhc} output}
  \label{f:simm5}
\end{subfigure}%
\begin{subfigure}{.167\textwidth}
  \centering
  \includegraphics[width=\linewidth]{kmeans_failure_2c.png}
  \caption{$k$-means output}
  \label{f:simm6}
\end{subfigure}
\begin{subfigure}{.167\textwidth}
  \centering
  \includegraphics[width=\linewidth]{kmeans_failure_3a.png}
  \caption{A point set}
  \label{f:simm7}
\end{subfigure}%
\begin{subfigure}{.167\textwidth}
  \centering
  \includegraphics[width=\linewidth]{kmeans_failure_3b.png}
  \caption{Algorithm~\ref{a:dhc} output}
  \label{f:simm8}
\end{subfigure}%
\begin{subfigure}{.167\textwidth}
  \centering
  \includegraphics[width=\linewidth]{kmeans_failure_3c.png}
  \caption{$k$-means output}
  \label{f:simm9}
\end{subfigure}
\begin{subfigure}{.167\textwidth}
  \centering
  \includegraphics[width=\linewidth]{kmeans_failure_4a.png}
  \caption{A point set}
  \label{f:simm10}
\end{subfigure}%
\begin{subfigure}{.167\textwidth}
  \centering
  \includegraphics[width=\linewidth]{kmeans_failure_4b.png}
  \caption{Algorithm~\ref{a:dhc} output}
  \label{f:simm11}
\end{subfigure}%
\begin{subfigure}{.167\textwidth}
  \centering
  \includegraphics[width=\linewidth]{kmeans_failure_4c.png}
  \caption{$k$-means output}
  \label{f:simm12}
\end{subfigure}
\begin{subfigure}{.167\textwidth}
  \centering
  \includegraphics[width=\linewidth]{kmeans_failure_6a.png}
  \caption{A point set}
  \label{f:simm13}
\end{subfigure}%
\begin{subfigure}{.167\textwidth}
  \centering
  \includegraphics[width=\linewidth]{kmeans_failure_6b.png}
  \caption{Algorithm~\ref{a:dhc} output}
  \label{f:simm14}
\end{subfigure}%
\begin{subfigure}{.167\textwidth}
  \centering
  \includegraphics[width=\linewidth]{kmeans_failure_6c.png}
  \caption{$k$-means output}
  \label{f:simm15}
\end{subfigure}
\begin{subfigure}{.167\textwidth}
  \centering
  \includegraphics[width=\linewidth]{kmeans_failure_5a.png}
  \caption{A point set}
  \label{f:simm16}
\end{subfigure}%
\begin{subfigure}{.167\textwidth}
  \centering
  \includegraphics[width=\linewidth]{kmeans_failure_5b.png}
  \caption{Algorithm~\ref{a:dhc} output}
  \label{f:simm17}
\end{subfigure}%
\begin{subfigure}{.167\textwidth}
  \centering
  \includegraphics[width=\linewidth]{kmeans_failure_5c.png}
  \caption{$k$-means output}
  \label{f:simm18}
\end{subfigure}
\caption{Some cases where $k$-means clustering fails to generate proper clusters but our algorithm excels.}
\label{f:kmeanfailure}
\end{figure}

We run our algorithm using both euclidean and manhattan distance metrics and $k$-means clustering on these datasets. Table~\ref{t:comp2} shows the results and comparison of our algorithm and $k$-means clustering. It is shown from \cite{franti2018k} that $k$-means performs poorly when cluster sizes have a strong unbalance. We find from the table that the difference of accuracy between our algorithm and $k$-means clustering is huge with the Unbalance dataset as our algorithm performs very well with this dataset but $k$-means performs poorly. On A-Sets, our algorithm with euclidean distance metric has the best accuracy among all. In the Aggregation dataset the clusters are well separated from one another and our algorithm with manhattan distance metric outperforms all with this dataset as manhattan distance usually yields robust results but unusual values influence euclidean distance. We also find from the table that $k$-means also performs worse than our algorithm with the Flame dataset. The only dataset that $k$-means performs slightly better than our algorithm is with the S$1$ dataset. In average, our algorithm with euclidean distance metric has the best accuracy among all. Fig.~\ref{f;comp1} shows the comparison plot of our algorithm with two different distance metrics and $k$-means clustering where the data are taken from Table~\ref{t:comp2}.

\begin{table}[!t]
\centering
\caption{Comparison of Algorithm~\ref{a:dhc} and $k$-means Clustering for the Examples of Fig.~\ref{f:mcoutput}.}\label{t:comp1}
\begin{tabular}{|c|c|c|c|c|c|}
\hline
\multirow{2}{*}{Example} & \multirow{2}{*}{Total Points} & \multicolumn{2}{c|}{RI} & \multicolumn{2}{c|}{ARI}\\ \cline{3-6}
& & $k$-means & Alg.~\ref{a:dhc} & $k$-means & Alg.~\ref{a:dhc}\\
\hline
$1$ & $10$ & $0.4444$ & $1.00$ & $-0.1194$ & $1.00$\\
$2$ & $10$ & $0.6444$ & $1.00$ & $0.28$ & $1.00$\\
$3$ & $20$ & $0.9$ & $1.00$ & $0.7996$ & $1.00$\\
$4$ & $38$ & $0.4865$ & $1.00$ & $-0.0277$ & $1.00$\\
$5$ & $36$ & $0.5873$ & $1.00$ & $0.1814$ & $1.00$\\
$6$ & $110$ & $0.9615$ & $1.00$ & $0.9227$ & $1.00$\\
\hline
\end{tabular}
\end{table}

\begin{table}[!t]
\centering
\caption{Comparison of Algorithm~\ref{a:dhc} and $k$-means Clustering for Different Benchmark Datasets.}\label{t:comp2}
\setlength\tabcolsep{5pt}
\begin{tabular}{|c|c|c|c|c|c|}
\hline
\multirow{3}{*}{Dataset} & \multirow{3}{*}{Total Points} & \multirow{3}{*}{$k$} & \multicolumn{3}{c|}{ARI} \\ \cline{4-6}
& & & \multicolumn{2}{c|}{Alg.~\ref{a:dhc}} & \multirow{2}{*}{$k$-means}\\ \cline{4 - 5}
& & & Euclidean & Manhattan &\\
\hline
Aggregation & $788$ & $7$ & $0.8133$ & $0.8561$ & $0.7952$\\
S1 & $5000$ & $15$ & $0.8341$ & $0.8214$ & $0.8523$\\
Unbalance & $6500$ & $8$ & $0.945$ & $0.8898$ & $0.6478$\\
Flame & $240$ & $2$ & $0.762$ & $0.7475$ & $0.4901$\\
A1 & $3000$ & $20$ & $0.8724$ & $0.8314$ & $0.826$\\
A2 & $5250$ & $35$ & $0.849$ & $0.8158$ & $0.8277$\\
A3 & $7500$ & $50$ & $0.8573$ & $0.7835$ & $0.8213$\\
\hline
\end{tabular}
\end{table}

\begin{figure}[!b]
\centering
\includegraphics[width=0.45\textwidth]{comparison_plot_ari.png}
\caption{Performance of Algorithm~\ref{a:dhc} and $k$-means clustering for different benchmark datasets.} \label{f;comp1}
\end{figure}

The quality of $k$-means clustering also depends on the value of $k$. It is shown from \cite{franti2018k} that the success of $k$-means clustering has an inverse linear dependency on $k$. We experiment the performance of our algorithm with euclidean distance metric and $k$-means clustering for increasing value of $k$. We have chosen the Birch$2$ dataset for this purpose which contains $100000$ data points. We run both our algorithm and $k$-means for $k = 10$ to $100$ for an interval of $10$ clusters on this benchmark dataset. We plot the results and it is shown in Fig.~\ref{f;com2}. We see that the accuracy of $k$-means gradually decreases with increasing value of $k$ which is also described in \cite{franti2018k}. The performance of our algorithm also decreases with increasing $k$ but still has a better accuracy compared to that of $k$-means for all values of $k$.

\begin{figure}[!t]
\centering
\includegraphics[width=0.45\textwidth]{comparison_plot_graph_ari.png}
\caption{Performance of our Algorithm~\ref{a:dhc} and $k$-means clustering for increasing number of clusters on Birch2 dataset.} \label{f;com2}
\end{figure}

\section{Conclusions} \label{s:fw}
In this paper, we presented a new and efficient divisive hierarchical clustering algorithm in to minimize the maximum diameter to cardinality ratio among all clusters. We focused on point set comprised of $2$-dimensional data points. %For maximum $k$ clusters allowed and given an input point set consisting of $n$ points, we proved that our algorithm has a running time complexity of $O(nk \log n)$.
Our algorithm has basically four steps those repeat in every iteration. It follows a greedy heuristic to complete one of these steps. After each iteration, it also follows the optimization criterion to select a cluster from the remaining clusters to split it and create two new clusters.

Experimental results validates the efficiency of our algorithm. We compared our algorithm with the widely popular $k$-means clustering. We discussed the cases where $k$-means performs poorly. We also showed some cases experimentally where $k$-means clustering fails but our algorithm excels.

We compared our algorithm's performance with $k$-means clustering on some benchmark datasets. These datasets differ in size and shape. We compared the performance of our algorithm with $k$-means clustering on these datasets using two different distance metrics and find that our algorithm with euclidean distance metric has the best efficiency among all these. We then tested both algorithms' accuracy with increasing value of $k$ and find that our algorithm performs better than $k$-means clustering.

We focused on $2$-dimensional points in this paper. But our algorithm can be extended for higher dimensions. 
%Analyzing the running time and evaluating the performance of our algorithm for higher dimensions would be an interesting problem.
We tested our algorithm using only euclidean and manhattan distances. But our algorithm can also be tested on other similarity metrics such as cosine similarity and mahalanobis distance. 
%Euclidean distance does not differentiate between correlated and uncorrelated dimensions but mahalanobis distance takes account of the correlation between the dimensions of the data and that is why it is widely used in machine learning and pattern recognition.

Finding a better way to overcome the problem of decreasing accuracy with increasing number of clusters would be a great job. As the real world data is huge and can be really complex in structure, higher number of clusters are often needed to be produced.

Finally testing accuracy on different datasets can find more clues about how to increase the quality of our algorithm. Similarity based on distance measure really do not work on some datasets such as some specific image data and audio data etc. In these cases, we need to test our algorithm with completely new kinds of similarity metrics and see whether these metrics actually work with the flow of our algorithm.

% \section{Ease of Use}
% \subsection{Maintaining the Integrity of the Specifications}
% The IEEEtran class file is used to format your paper and style the text. All margins, 
% column widths, line spaces, and text fonts are prescribed; please do not 
% alter them. You may note peculiarities. For example, the head margin
% measures proportionately more than is customary. This measurement 
% and others are deliberate, using specifications that anticipate your paper 
% as one part of the entire proceedings, and not as an independent document. 
% Please do not revise any of the current designations.

% \section{Prepare Your Paper Before Styling}
% Before you begin to format your paper, first write and save the content as a 
% separate text file. Complete all content and organizational editing before 
% formatting. Please note sections \ref{AA}--\ref{SCM} below for more information on 
% proofreading, spelling and grammar.

% Keep your text and graphic files separate until after the text has been 
% formatted and styled. Do not number text heads---{\LaTeX} will do that 
% for you.

% \subsection{Abbreviations and Acronyms}\label{AA}
% Define abbreviations and acronyms the first time they are used in the text, 
% even after they have been defined in the abstract. Abbreviations such as 
% IEEE, SI, MKS, CGS, ac, dc, and rms do not have to be defined. Do not use 
% abbreviations in the title or heads unless they are unavoidable.

% \subsection{Units}
% \begin{itemize}
% \item Use either SI (MKS) or CGS as primary units. (SI units are encouraged.) English units may be used as secondary units (in parentheses). An exception would be the use of English units as identifiers in trade, such as ``3.5-inch disk drive''.
% \item Avoid combining SI and CGS units, such as current in amperes and magnetic field in oersteds. This often leads to confusion because equations do not balance dimensionally. If you must use mixed units, clearly state the units for each quantity that you use in an equation.
% \item Do not mix complete spellings and abbreviations of units: ``Wb/m\textsuperscript{2}'' or ``webers per square meter'', not ``webers/m\textsuperscript{2}''. Spell out units when they appear in text: ``. . . a few henries'', not ``. . . a few H''.
% \item Use a zero before decimal points: ``0.25'', not ``.25''. Use ``cm\textsuperscript{3}'', not ``cc''.)
% \end{itemize}

% \subsection{Equations}
% Number equations consecutively. To make your 
% equations more compact, you may use the solidus (~/~), the exp function, or 
% appropriate exponents. Italicize Roman symbols for quantities and variables, 
% but not Greek symbols. Use a long dash rather than a hyphen for a minus 
% sign. Punctuate equations with commas or periods when they are part of a 
% sentence, as in:
% \begin{equation}
% a+b=\gamma\label{eq}
% \end{equation}

% Be sure that the 
% symbols in your equation have been defined before or immediately following 
% the equation. Use ``\eqref{eq}'', not ``Eq.~\eqref{eq}'' or ``equation \eqref{eq}'', except at 
% the beginning of a sentence: ``Equation \eqref{eq} is . . .''

% \subsection{\LaTeX-Specific Advice}

% Please use ``soft'' (e.g., \verb|\eqref{Eq}|) cross references instead
% of ``hard'' references (e.g., \verb|(1)|). That will make it possible
% to combine sections, add equations, or change the order of figures or
% citations without having to go through the file line by line.

% Please don't use the \verb|{eqnarray}| equation environment. Use
% \verb|{align}| or \verb|{IEEEeqnarray}| instead. The \verb|{eqnarray}|
% environment leaves unsightly spaces around relation symbols.

% Please note that the \verb|{subequations}| environment in {\LaTeX}
% will increment the main equation counter even when there are no
% equation numbers displayed. If you forget that, you might write an
% article in which the equation numbers skip from (17) to (20), causing
% the copy editors to wonder if you've discovered a new method of
% counting.

% {\BibTeX} does not work by magic. It doesn't get the bibliographic
% data from thin air but from .bib files. If you use {\BibTeX} to produce a
% bibliography you must send the .bib files. 

% {\LaTeX} can't read your mind. If you assign the same label to a
% subsubsection and a table, you might find that Table I has been cross
% referenced as Table IV-B3. 

% {\LaTeX} does not have precognitive abilities. If you put a
% \verb|\label| command before the command that updates the counter it's
% supposed to be using, the label will pick up the last counter to be
% cross referenced instead. In particular, a \verb|\label| command
% should not go before the caption of a figure or a table.

% Do not use \verb|\nonumber| inside the \verb|{array}| environment. It
% will not stop equation numbers inside \verb|{array}| (there won't be
% any anyway) and it might stop a wanted equation number in the
% surrounding equation.

% \subsection{Some Common Mistakes}\label{SCM}
% \begin{itemize}
% \item The word ``data'' is plural, not singular.
% \item The subscript for the permeability of vacuum $\mu_{0}$, and other common scientific constants, is zero with subscript formatting, not a lowercase letter ``o''.
% \item In American English, commas, semicolons, periods, question and exclamation marks are located within quotation marks only when a complete thought or name is cited, such as a title or full quotation. When quotation marks are used, instead of a bold or italic typeface, to highlight a word or phrase, punctuation should appear outside of the quotation marks. A parenthetical phrase or statement at the end of a sentence is punctuated outside of the closing parenthesis (like this). (A parenthetical sentence is punctuated within the parentheses.)
% \item A graph within a graph is an ``inset'', not an ``insert''. The word alternatively is preferred to the word ``alternately'' (unless you really mean something that alternates).
% \item Do not use the word ``essentially'' to mean ``approximately'' or ``effectively''.
% \item In your paper title, if the words ``that uses'' can accurately replace the word ``using'', capitalize the ``u''; if not, keep using lower-cased.
% \item Be aware of the different meanings of the homophones ``affect'' and ``effect'', ``complement'' and ``compliment'', ``discreet'' and ``discrete'', ``principal'' and ``principle''.
% \item Do not confuse ``imply'' and ``infer''.
% \item The prefix ``non'' is not a word; it should be joined to the word it modifies, usually without a hyphen.
% \item There is no period after the ``et'' in the Latin abbreviation ``et al.''.
% \item The abbreviation ``i.e.'' means ``that is'', and the abbreviation ``e.g.'' means ``for example''.
% \end{itemize}
% An excellent style manual for science writers is \cite{b7}.

% \subsection{Authors and Affiliations}
% \textbf{The class file is designed for, but not limited to, six authors.} A 
% minimum of one author is required for all conference articles. Author names 
% should be listed starting from left to right and then moving down to the 
% next line. This is the author sequence that will be used in future citations 
% and by indexing services. Names should not be listed in columns nor group by 
% affiliation. Please keep your affiliations as succinct as possible (for 
% example, do not differentiate among departments of the same organization).

% \subsection{Identify the Headings}
% Headings, or heads, are organizational devices that guide the reader through 
% your paper. There are two types: component heads and text heads.

% Component heads identify the different components of your paper and are not 
% topically subordinate to each other. Examples include Acknowledgments and 
% References and, for these, the correct style to use is ``Heading 5''. Use 
% ``figure caption'' for your Figure captions, and ``table head'' for your 
% table title. Run-in heads, such as ``Abstract'', will require you to apply a 
% style (in this case, italic) in addition to the style provided by the drop 
% down menu to differentiate the head from the text.

% Text heads organize the topics on a relational, hierarchical basis. For 
% example, the paper title is the primary text head because all subsequent 
% material relates and elaborates on this one topic. If there are two or more 
% sub-topics, the next level head (uppercase Roman numerals) should be used 
% and, conversely, if there are not at least two sub-topics, then no subheads 
% should be introduced.

% \subsection{Figures and Tables}
% \paragraph{Positioning Figures and Tables} Place figures and tables at the top and 
% bottom of columns. Avoid placing them in the middle of columns. Large 
% figures and tables may span across both columns. Figure captions should be 
% below the figures; table heads should appear above the tables. Insert 
% figures and tables after they are cited in the text. Use the abbreviation 
% ``Fig.~\ref{fig}'', even at the beginning of a sentence.

% \begin{table}[htbp]
% \caption{Table Type Styles}
% \begin{center}
% \begin{tabular}{|c|c|c|c|}
% \hline
% \textbf{Table}&\multicolumn{3}{|c|}{\textbf{Table Column Head}} \\
% \cline{2-4} 
% \textbf{Head} & \textbf{\textit{Table column subhead}}& \textbf{\textit{Subhead}}& \textbf{\textit{Subhead}} \\
% \hline
% copy& More table copy$^{\mathrm{a}}$& &  \\
% \hline
% \multicolumn{4}{l}{$^{\mathrm{a}}$Sample of a Table footnote.}
% \end{tabular}
% \label{tab1}
% \end{center}
% \end{table}

% \begin{figure}[htbp]
% \centerline{\includegraphics{fig1.png}}
% \caption{Example of a figure caption.}
% \label{fig}
% \end{figure}

% Figure Labels: Use 8 point Times New Roman for Figure labels. Use words 
% rather than symbols or abbreviations when writing Figure axis labels to 
% avoid confusing the reader. As an example, write the quantity 
% ``Magnetization'', or ``Magnetization, M'', not just ``M''. If including 
% units in the label, present them within parentheses. Do not label axes only 
% with units. In the example, write ``Magnetization (A/m)'' or ``Magnetization 
% \{A[m(1)]\}'', not just ``A/m''. Do not label axes with a ratio of 
% quantities and units. For example, write ``Temperature (K)'', not 
% ``Temperature/K''.

% \section*{Acknowledgment}

% The preferred spelling of the word ``acknowledgment'' in America is without 
% an ``e'' after the ``g''. Avoid the stilted expression ``one of us (R. B. 
% G.) thanks $\ldots$''. Instead, try ``R. B. G. thanks$\ldots$''. Put sponsor 
% acknowledgments in the unnumbered footnote on the first page.

% \section*{References}

% Please number citations consecutively within brackets \cite{jain2010data}. The 
% sentence punctuation follows the bracket \cite{b2}. Refer simply to the reference 
% number, as in \cite{b3}---do not use ``Ref. \cite{b3}'' or ``reference \cite{b3}'' except at 
% the beginning of a sentence: ``Reference \cite{b3} was the first $\ldots$''

% Number footnotes separately in superscripts. Place the actual footnote at 
% the bottom of the column in which it was cited. Do not put footnotes in the 
% abstract or reference list. Use letters for table footnotes.

% Unless there are six authors or more give all authors' names; do not use 
% ``et al.''. Papers that have not been published, even if they have been 
% submitted for publication, should be cited as ``unpublished'' \cite{b4}. Papers 
% that have been accepted for publication should be cited as ``in press'' \cite{b5}. 
% Capitalize only the first word in a paper title, except for proper nouns and 
% element symbols.

% For papers published in translation journals, please give the English 
% citation first, followed by the original foreign-language citation \cite{b6}.

% \begin{thebibliography}{00}
% \bibitem{b1} G. Eason, B. Noble, and I. N. Sneddon, ``On certain integrals of Lipschitz-Hankel type involving products of Bessel functions,'' Phil. Trans. Roy. Soc. London, vol. A247, pp. 529--551, April 1955.
% \bibitem{b2} J. Clerk Maxwell, A Treatise on Electricity and Magnetism, 3rd ed., vol. 2. Oxford: Clarendon, 1892, pp.68--73.
% \bibitem{b3} I. S. Jacobs and C. P. Bean, ``Fine particles, thin films and exchange anisotropy,'' in Magnetism, vol. III, G. T. Rado and H. Suhl, Eds. New York: Academic, 1963, pp. 271--350.
% \bibitem{b4} K. Elissa, ``Title of paper if known,'' unpublished.
% \bibitem{b5} R. Nicole, ``Title of paper with only first word capitalized,'' J. Name Stand. Abbrev., in press.
% \bibitem{b6} Y. Yorozu, M. Hirano, K. Oka, and Y. Tagawa, ``Electron spectroscopy studies on magneto-optical media and plastic substrate interface,'' IEEE Transl. J. Magn. Japan, vol. 2, pp. 740--741, August 1987 [Digests 9th Annual Conf. Magnetics Japan, p. 301, 1982].
% \bibitem{b7} M. Young, The Technical Writer's Handbook. Mill Valley, CA: University Science, 1989.
% \end{thebibliography}

\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv,refs}

% \vspace{12pt}
% \color{red}
% IEEE conference templates contain guidance text for composing and formatting conference papers. Please ensure that all template text is removed from your conference paper prior to submission to the conference. Failure to remove the template text from your paper may result in your paper not being published.

\end{document}
